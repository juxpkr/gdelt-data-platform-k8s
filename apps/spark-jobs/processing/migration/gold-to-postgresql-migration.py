import os
import sys
import logging
from pathlib import Path

# sys.path.append ëŒ€ì‹ , ì´ íŒŒì¼ì˜ ìœ„ì¹˜ë¥¼ ê¸°ì¤€ìœ¼ë¡œ í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ ì°¾ì•„ì„œ ê²½ë¡œì— ì¶”ê°€
project_root = Path(__file__).resolve().parents[3]
sys.path.append(str(project_root))

from utils.spark_builder import get_spark_session
from pyspark.sql import SparkSession, DataFrame

logging.basicConfig(
    level=logging.INFO, format="%(asctime)s [%(levelname)s] %(message)s"
)
logger = logging.getLogger(__name__)


def read_gold_table(
    spark: SparkSession, table_name: str, table_path: str
) -> DataFrame | None:
    """
    Gold Layer í…Œì´ë¸”ì„ ì½ì–´ DataFrameìœ¼ë¡œ ë°˜í™˜í•œë‹¤.
    Metastoreì—ì„œ ë¨¼ì € ì°¾ê³ , ì‹¤íŒ¨ ì‹œ S3 ê²½ë¡œì—ì„œ ì§ì ‘ ì½ëŠ”ë‹¤.
    """
    logger.info(f"ğŸ“¥ Reading Gold table '{table_name}' from Hive Metastore...")
    try:
        # 1. Hive Metastoreë¥¼ í†µí•´ í…Œì´ë¸” ì½ê¸°
        gold_df = spark.table(table_name)
        logger.info(f"âœ… Successfully read from Metastore.")
        return gold_df
    except Exception:
        logger.warning(
            f"âš ï¸ Could not find table '{table_name}' in Metastore. "
            f"Attempting to read directly from Delta path: {table_path}"
        )
        try:
            # 2. S3 ê²½ë¡œì—ì„œ ì§ì ‘ Delta íŒŒì¼ ì½ê¸°
            gold_df = spark.read.format("delta").load(table_path)
            logger.info(f"âœ… Successfully read from S3 path.")
            return gold_df
        except Exception as e:
            logger.error(
                f"âŒ Failed to read Gold data from both Metastore and S3 path.",
                exc_info=True,
            )
            return None


def write_to_postgres(df: DataFrame, dbtable: str):
    # DataFrameì„ PostgreSQL í…Œì´ë¸”ì— ë®ì–´ì“´ë‹¤.
    postgres_host = os.getenv("POSTGRES_HOST", "postgres")
    postgres_port = os.getenv("POSTGRES_PORT", "5432")
    # Gold ë°ì´í„°ì˜ ìµœì¢… ëª©ì ì§€ëŠ” Airflow DBë¡œ ì§€ì •
    postgres_db = os.getenv("POSTGRES_DB", "airflow")
    pg_url = f"jdbc:postgresql://{postgres_host}:{postgres_port}/{postgres_db}"

    logger.info(
        f"ğŸ’¾ Writing {df.count()} records to PostgreSQL table '{dbtable}' at {pg_url}..."
    )

    (
        df.write.format("jdbc")
        .option("url", pg_url)
        .option("dbtable", dbtable)
        .option("user", os.getenv("POSTGRES_USER", "airflow"))
        .option("password", os.getenv("POSTGRES_PASSWORD", "airflow"))
        .option("driver", "org.postgresql.Driver")
        .mode("overwrite")
        .save()
    )
    logger.info(f"âœ… Migration completed successfully to table '{dbtable}'.")


def get_gold_tables_from_minio(spark: SparkSession, base_path: str) -> list[str]:
    """
    ì£¼ì–´ì§„ S3 ê²½ë¡œì—ì„œ ëª¨ë“  í•˜ìœ„ í´ë”(Gold í…Œì´ë¸”)ì˜ ëª©ë¡ì„ ê°€ì ¸ì˜µë‹ˆë‹¤.
    """
    logger.info(f"ğŸ” MinIO ê²½ë¡œ '{base_path}'ì—ì„œ Gold í…Œì´ë¸” ëª©ë¡ì„ ê²€ìƒ‰ ì¤‘...")
    try:
        s3_uri = spark._jvm.java.net.URI.create(base_path)
        fs = spark._jvm.org.apache.hadoop.fs.FileSystem.get(
            s3_uri, spark._jsc.hadoopConfiguration()
        )
        status = fs.listStatus(spark._jvm.org.apache.hadoop.fs.Path(base_path))

        gold_tables = [f.getPath().getName() for f in status if f.isDirectory()]

        if not gold_tables:
            logger.warning(
                f"âš ï¸ í•´ë‹¹ ê²½ë¡œì—ì„œ Gold í…Œì´ë¸” í´ë”ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {base_path}"
            )
            return []

        logger.info(
            f"âœ… {len(gold_tables)}ê°œì˜ Gold í…Œì´ë¸”ì„ ì°¾ì•˜ìŠµë‹ˆë‹¤: {gold_tables}"
        )
        return gold_tables
    except Exception as e:
        logger.error(f"âŒ MinIO ê²½ë¡œë¥¼ ì½ëŠ” ì¤‘ ì—ëŸ¬ ë°œìƒ: {base_path}", exc_info=True)
        return []


def main():
    # ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜
    logger.info("ğŸš€ Starting Gold to PostgreSQL Migration...")
    spark = get_spark_session(
        "Gold_To_PostgreSQL_Migration", "spark://spark-master:7077"
    )

    try:
        # # 1. Gold í…Œì´ë¸” ì½ê¸°
        # gold_table_name = "gold.gold_4th_daily_detail_summary"
        # gold_table_path = "s3a://warehouse/gold/gold_4th_daily_detail_summary"
        # gold_df = read_gold_table(spark, gold_table_name, gold_table_path)

        # if gold_df is None or gold_df.rdd.isEmpty():
        #     logger.warning("âš ï¸ No data found in Gold table. Exiting gracefully.")
        #     return

        # # 2. PostgreSQLì— ì“°ê¸°
        # write_to_postgres(gold_df, "gold_4th_daily_detail_summary")

        gold_base_path = "s3a://warehouse/gold/"
        gold_tables = get_gold_tables_from_minio(spark, gold_base_path)

        if not gold_tables:
            logger.info("ì²˜ë¦¬í•  Gold í…Œì´ë¸”ì´ ì—†ìŠµë‹ˆë‹¤. ì‘ì—…ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
            return

        for table_folder_name in gold_tables:
            logger.info(f"\n{'='*20} [{table_folder_name}] í…Œì´ë¸” ì²˜ë¦¬ ì‹œì‘ {'='*20}")

            # 1. Gold í…Œì´ë¸” ì´ë¦„ê³¼ ê²½ë¡œë¥¼ ë™ì ìœ¼ë¡œ ìƒì„±
            gold_table_name = f"gold.{table_folder_name}"
            gold_table_path_s3 = f"{gold_base_path}{table_folder_name}"
            postgres_table_name = table_folder_name

            # 2. Gold í…Œì´ë¸” ì½ê¸°
            gold_df = read_gold_table(spark, gold_table_name, gold_table_path_s3)

            if gold_df is None or gold_df.rdd.isEmpty():
                logger.warning(
                    f"âš ï¸ í…Œì´ë¸” '{table_folder_name}'ì— ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. ë‹¤ìŒ í…Œì´ë¸”ë¡œ ë„˜ì–´ê°‘ë‹ˆë‹¤."
                )
                continue

            # 3. PostgreSQLì— ì“°ê¸°
            write_to_postgres(gold_df, postgres_table_name)

    except Exception as e:
        logger.error(
            f"âŒ A critical error occurred in the migration pipeline: {e}",
            exc_info=True,
        )
        sys.exit(1)
    finally:
        spark.stop()
        logger.info("âœ… Spark session closed")


if __name__ == "__main__":
    main()
