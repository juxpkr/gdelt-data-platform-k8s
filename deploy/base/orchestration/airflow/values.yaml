# Airflow 이미지 버전
images:
  airflow:
    repository: juxpkr/geoevent-airflow
    tag: "2.2-delta"
    pullPolicy: IfNotPresent

# 전체 컴포넌트 공통 환경변수
env:
  # Kafka 설정
  - name: KAFKA_BOOTSTRAP_SERVERS
    value: "kafka-headless.airflow.svc.cluster.local:9092"
  - name: KAFKA_TOPIC_GDELT_EVENTS
    value: "gdelt_events_bronze"
  - name: KAFKA_TOPIC_GDELT_MENTIONS
    value: "gdelt_mentions_bronze"
  - name: KAFKA_TOPIC_GDELT_GKG
    value: "gdelt_gkg_bronze"

  # MinIO 설정
  - name: MINIO_ENDPOINT
    value: "http://minio.airflow.svc.cluster.local:9000"
  - name: MINIO_ROOT_USER
    value: "minioadmin"
  - name: MINIO_ROOT_PASSWORD
    value: "minioadmin"

  # Redis 설정
  - name: REDIS_HOST
    value: "airflow-redis"
  - name: REDIS_PORT
    value: "6379"

  # PostgreSQL 설정 (metastore)
  - name: POSTGRES_HOST
    value: "postgres-custom.airflow.svc.cluster.local"
  - name: POSTGRES_PORT
    value: "5432"
  - name: POSTGRES_USER
    value: "postgres"
  - name: POSTGRES_PASSWORD
    value: "postgres"
  - name: POSTGRES_DB
    value: "metastore_db"

  # Spark Connection
  - name: AIRFLOW_CONN_SPARK_DEFAULT
    value: "spark://local"
extraEnv: |
  - name: REDIS_PASSWORD
    valueFrom:
      secretKeyRef:
        name: airflow-redis-password
        key: password

# 실행 방식
executor: CeleryExecutor

# 컴포넌트별 리소스 다이어트(16GB RAM)
scheduler:
  replicas: 1
  resources:
    requests:
      cpu: "100m"
      memory: "256Mi"
    limits:
      cpu: "500m"
      memory: "512Mi"

  # DAGs 볼륨 마운트
  extraVolumes:
    - name: dags-data
      hostPath:
        path: /home/tajo/gdelt-data-platform-k8s/apps/airflow/dags
        type: Directory
    - name: plugins-data
      hostPath:
        path: /home/tajo/gdelt-data-platform-k8s/apps/airflow/plugins
        type: Directory
    - name: jobs-data
      hostPath:
        path: /home/tajo/gdelt-data-platform-k8s/apps/spark-jobs
        type: Directory

  extraVolumeMounts:
    - name: dags-data
      mountPath: /opt/airflow/dags
      readOnly: true
    - name: plugins-data
      mountPath: /opt/airflow/plugins
      readOnly: true
    - name: jobs-data
      mountPath: /opt/airflow/spark-jobs
      readOnly: true

webserver:
  replicas: 1
  resources:
    requests:
      cpu: "100m"
      memory: "512Mi"
    limits:
      cpu: "500m"
      memory: "1Gi"

  # DAGs 볼륨 마운트
  extraVolumes:
    - name: dags-data
      hostPath:
        path: /home/tajo/gdelt-data-platform-k8s/apps/airflow/dags
        type: Directory
    - name: plugins-data
      hostPath:
        path: /home/tajo/gdelt-data-platform-k8s/apps/airflow/plugins
        type: Directory
    - name: jobs-data
      hostPath:
        path: /home/tajo/gdelt-data-platform-k8s/apps/spark-jobs
        type: Directory

  extraVolumeMounts:
    - name: dags-data
      mountPath: /opt/airflow/dags
      readOnly: true
    - name: plugins-data
      mountPath: /opt/airflow/plugins
      readOnly: true
    - name: jobs-data
      mountPath: /opt/airflow/spark-jobs
      readOnly: true

  # Gunicorn 워커 수 줄이기 (4개 -> 2개)
  # 리소스 절약 + 부팅 속도 향상
  env:
    - name: AIRFLOW__WEBSERVER__WORKERS
      value: "2"
    - name: AIRFLOW__WEBSERVER__WORKER_TIMEOUT
      value: "180"

  # 시동 검사
  startupProbe:
    failureThreshold: 30     # 30번 시도
    periodSeconds: 10         # 10초마다 (총 300초)
    timeoutSeconds: 5
  
  # 생존 검사
  livenessProbe:
    initialDelaySeconds: 10
    timeoutSeconds: 30
    failureThreshold: 5
    periodSeconds: 30

  # 준비 검사
  readinessProbe:
    initialDelaySeconds: 10
    timeoutSeconds: 30
    failureThreshold: 5
    periodSeconds: 30

workers:
  replicas: 1
  resources:
    requests:
      cpu: "50m"
      memory: "512Mi"
    limits:
      cpu: "1000m"
      memory: "6Gi"

  # DAGs 볼륨 마운트
  extraVolumes:
    - name: dags-data
      hostPath:
        path: /home/tajo/gdelt-data-platform-k8s/apps/airflow/dags
        type: Directory
    - name: plugins-data
      hostPath:
        path: /home/tajo/gdelt-data-platform-k8s/apps/airflow/plugins
        type: Directory
    - name: jobs-data
      hostPath:
        path: /home/tajo/gdelt-data-platform-k8s/apps/spark-jobs
        type: Directory

  extraVolumeMounts:
    - name: dags-data
      mountPath: /opt/airflow/dags
      readOnly: true
    - name: plugins-data
      mountPath: /opt/airflow/plugins
      readOnly: true
    - name: jobs-data
      mountPath: /opt/airflow/spark-jobs
      readOnly: true

  tolerations:
    - key: "node-role.kubernetes.io/control-plane"
      operator: "Exists"
      effect: "NoSchedule"

# 안 쓰는 기능 끄기 - 메모리 확보
triggerer:
  enabled: false
statsd:
  enabled: false

# Redis 다이어트
redis:
  enabled: true
  resources:
    requests:
      cpu: "50m"
      memory: "64Mi"
    limits:
      cpu: "100m"
      memory: "128Mi"

# 외부 DB 연결
postgresql:
  enabled: false  # 내장 DB 비활성화
data:
  metadataConnection:
    user: postgres
    pass: postgres
    host: postgres-custom
    port: 5432
    db: airflow
    protocol: postgresql

# 동시성 제어
config:
  core:
    parallelism: 2 # 전체에서 동시에 딱 2개 태스크만 실행
    max_active_tasks_per_dag: 2 # DAG당 2개만
    max_active_runs_per_dag: 1 # 배치가 밀려도 중첩 실행 안 됨
    load_examples: 'False' # 예제 DAG 로딩 끔
    catchup_by_default: 'False' # 과거 데이터 소급 실행 방지

  celery:
    worker_concurrency: 2  # 2개의 프로세스만 실행


# 외부 접속 (Ingress)
ingress:
  web:
    enabled: true
    ingressClassName: "nginx"
    hosts:
      - name: ""

# DAGs 폴더 설정 (HostPath 방식)
dags:
  persistence:
    enabled: false # PVC 안 쓰고 직접 마운트
  gitSync:
    enabled: false
